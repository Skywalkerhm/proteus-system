#!/usr/bin/env python3
"""
ğŸ§  Olympus LLM Integration - æ™ºèƒ½ä»»åŠ¡åˆ†è§£ä¸æ‰§è¡Œ

æ”¯æŒï¼š
- OpenAI API (GPT-4)
- Anthropic API (Claude)
- æœ¬åœ°æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆæ—  API key æ—¶ï¼‰

å®‰å…¨æç¤ºï¼š
- API key é€šè¿‡ç¯å¢ƒå˜é‡è·å–
- ä¸ä¼šç¡¬ç¼–ç åœ¨ä»£ç ä¸­
- æ”¯æŒ fallback åˆ°æ¨¡æ‹Ÿæ¨¡å¼
"""

import json
import os
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional


class LLMClient:
    """
    LLM å®¢æˆ·ç«¯
    
    æ”¯æŒå¤šç§ LLM æä¾›å•†ï¼Œè‡ªåŠ¨ fallback åˆ°æ¨¡æ‹Ÿæ¨¡å¼
    
    ç¯å¢ƒå˜é‡:
        OLYMPUS_LLM_PROVIDER: openai | anthropic | mock (default: mock)
        OPENAI_API_KEY: OpenAI API key
        ANTHROPIC_API_KEY: Anthropic API key
    """
    
    def __init__(self, provider: str = None, api_key: str = None):
        """
        åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        
        Args:
            provider: LLM æä¾›å•† (openai/anthropic/mock)
            api_key: API key (ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡)
        """
        # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
        self.provider = provider or os.getenv("OLYMPUS_LLM_PROVIDER", "mock")
        self.api_key = api_key or self._get_api_key()
        
        # å®¢æˆ·ç«¯å®ä¾‹
        self.openai_client = None
        self.anthropic_client = None
        
        # åˆå§‹åŒ–å¯¹åº”çš„å®¢æˆ·ç«¯
        self._initialize_client()
        
        print(f"ğŸ§  LLM Client å·²åˆå§‹åŒ–")
        print(f"   æä¾›å•†ï¼š{self.provider}")
        print(f"   API Key: {'å·²é…ç½®' if self.api_key else 'æœªé…ç½® (ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼)'}")
    
    def _get_api_key(self) -> Optional[str]:
        """å®‰å…¨è·å– API key"""
        if self.provider == "openai":
            return os.getenv("OPENAI_API_KEY")
        elif self.provider == "anthropic":
            return os.getenv("ANTHROPIC_API_KEY")
        return None
    
    def _initialize_client(self):
        """åˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
        if self.provider == "openai" and self.api_key:
            try:
                import openai
                self.openai_client = openai.OpenAI(api_key=self.api_key)
                print("   âœ… OpenAI å®¢æˆ·ç«¯å·²åˆå§‹åŒ–")
            except ImportError:
                print("   âš ï¸  openai åŒ…æœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                self.provider = "mock"
        
        elif self.provider == "anthropic" and self.api_key:
            try:
                import anthropic
                self.anthropic_client = anthropic.Anthropic(api_key=self.api_key)
                print("   âœ… Anthropic å®¢æˆ·ç«¯å·²åˆå§‹åŒ–")
            except ImportError:
                print("   âš ï¸  anthropic åŒ…æœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                self.provider = "mock"
    
    def decompose_task(self, task_desc: str, context: Dict = None) -> List[Dict]:
        """
        ä½¿ç”¨ LLM æ™ºèƒ½åˆ†è§£ä»»åŠ¡
        
        Args:
            task_desc: ä»»åŠ¡æè¿°
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
        
        Returns:
            å­ä»»åŠ¡åˆ—è¡¨
        """
        if self.provider in ["openai", "anthropic"] and self.api_key:
            try:
                return self._llm_decompose(task_desc, context)
            except Exception as e:
                print(f"   âš ï¸  LLM è°ƒç”¨å¤±è´¥ï¼š{e}")
                print("   ğŸ”„ Fallback åˆ°æ¨¡æ‹Ÿæ¨¡å¼")
                return self._mock_decompose(task_desc)
        else:
            return self._mock_decompose(task_desc)
    
    def _llm_decompose(self, task_desc: str, context: Dict = None) -> List[Dict]:
        """ä½¿ç”¨çœŸå® LLM åˆ†è§£ä»»åŠ¡"""
        
        # æ„å»ºæç¤ºè¯
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»»åŠ¡è§„åˆ’ä¸“å®¶ã€‚è¯·å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¯æ‰§è¡Œçš„å­ä»»åŠ¡ã€‚

æ¯ä¸ªå­ä»»åŠ¡å¿…é¡»åŒ…å«ï¼š
- desc: ä»»åŠ¡æè¿°ï¼ˆæ¸…æ™°å…·ä½“ï¼‰
- required_skills: æ‰€éœ€æŠ€èƒ½åˆ—è¡¨
- agent_type: é€‚åˆçš„ Agent ç±»å‹ (athena/hermes/apollo/hephaestus/muse/hestia/themis/aphrodite/echo/daedalus)
- estimated_time: é¢„ä¼°æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰

åªè¿”å› JSON æ•°ç»„ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

        user_prompt = f"""è¯·åˆ†è§£ä»¥ä¸‹ä»»åŠ¡ï¼š

ä»»åŠ¡ï¼š{task_desc}

{'ä¸Šä¸‹æ–‡ï¼š' + json.dumps(context, ensure_ascii=False) if context else ''}

è¯·è¿”å›å­ä»»åŠ¡åˆ—è¡¨ï¼ˆJSON æ•°ç»„æ ¼å¼ï¼‰ï¼š"""

        if self.provider == "openai" and self.openai_client:
            return self._call_openai(system_prompt, user_prompt)
        elif self.provider == "anthropic" and self.anthropic_client:
            return self._call_anthropic(system_prompt, user_prompt)
        else:
            return self._mock_decompose(task_desc)
    
    def _call_openai(self, system_prompt: str, user_prompt: str) -> List[Dict]:
        """è°ƒç”¨ OpenAI API"""
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.7,
                max_tokens=2000
            )
            
            content = response.choices[0].message.content.strip()
            
            # æå– JSON
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            
            subtasks = json.loads(content)
            
            # ç¡®ä¿æ ¼å¼æ­£ç¡®
            return self._validate_subtasks(subtasks)
            
        except Exception as e:
            print(f"OpenAI API è°ƒç”¨å¤±è´¥ï¼š{e}")
            raise
    
    def _call_anthropic(self, system_prompt: str, user_prompt: str) -> List[Dict]:
        """è°ƒç”¨ Anthropic API"""
        try:
            response = self.anthropic_client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=2000,
                system=system_prompt,
                messages=[
                    {"role": "user", "content": user_prompt}
                ]
            )
            
            content = response.content[0].text.strip()
            
            # æå– JSON
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            
            subtasks = json.loads(content)
            
            # ç¡®ä¿æ ¼å¼æ­£ç¡®
            return self._validate_subtasks(subtasks)
            
        except Exception as e:
            print(f"Anthropic API è°ƒç”¨å¤±è´¥ï¼š{e}")
            raise
    
    def _validate_subtasks(self, subtasks: List[Dict]) -> List[Dict]:
        """éªŒè¯å­ä»»åŠ¡æ ¼å¼"""
        validated = []
        for st in subtasks:
            validated.append({
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": st.get("desc", "æœªå‘½åä»»åŠ¡"),
                "required_skills": st.get("required_skills", ["general"]),
                "agent_type": st.get("agent_type", "hephaestus"),
                "estimated_time": st.get("estimated_time", 30),
                "status": "pending",
                "llm_generated": True
            })
        return validated
    
    def _mock_decompose(self, task_desc: str) -> List[Dict]:
        """æ¨¡æ‹Ÿä»»åŠ¡åˆ†è§£ï¼ˆfallbackï¼‰"""
        task_lower = task_desc.lower()
        
        if "ç¤¾äº¤åª’ä½“" in task_desc or "å†…å®¹è®¡åˆ’" in task_desc:
            return self._decompose_social_media(task_desc)
        elif "ç ”ç©¶" in task_desc or "æŠ¥å‘Š" in task_desc:
            return self._decompose_research(task_desc)
        elif "ä»£ç " in task_desc or "ç¼–ç¨‹" in task_desc:
            return self._decompose_coding(task_desc)
        elif "ç½‘ç«™" in task_desc or "å¼€å‘" in task_desc:
            return self._decompose_web_development(task_desc)
        else:
            return self._decompose_generic(task_desc)
    
    def _decompose_social_media(self, task_desc: str) -> List[Dict]:
        """ç¤¾äº¤åª’ä½“ä»»åŠ¡åˆ†è§£"""
        return [
            {
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": "è°ƒç ”ç›®æ ‡å—ä¼—å’Œè¡Œä¸šè¶‹åŠ¿",
                "required_skills": ["research", "analysis"],
                "agent_type": "athena",
                "estimated_time": 45,
                "status": "pending",
                "llm_generated": False
            },
            {
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": "åˆ¶å®šå†…å®¹ä¸»é¢˜å’Œå‘å¸ƒæ—¥å†",
                "required_skills": ["planning", "strategy", "social_media"],
                "agent_type": "apollo",
                "estimated_time": 30,
                "status": "pending",
                "llm_generated": False
            },
            {
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": "æ’°å†™æ¯æ—¥æ–‡æ¡ˆè‰ç¨¿",
                "required_skills": ["writing", "copywriting"],
                "agent_type": "apollo",
                "estimated_time": 90,
                "status": "pending",
                "llm_generated": False
            },
            {
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": "è®¾è®¡è§†è§‰é£æ ¼å’Œé…å›¾å»ºè®®",
                "required_skills": ["design", "visual"],
                "agent_type": "hephaestus",
                "estimated_time": 60,
                "status": "pending",
                "llm_generated": False
            },
            {
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": "è´¨é‡å®¡æ ¸ä¸ä¼˜åŒ–",
                "required_skills": ["review", "quality_control"],
                "agent_type": "themis",
                "estimated_time": 30,
                "status": "pending",
                "llm_generated": False
            }
        ]
    
    def _decompose_research(self, task_desc: str) -> List[Dict]:
        """ç ”ç©¶ä»»åŠ¡åˆ†è§£"""
        return [
            {
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": "å®šä¹‰ç ”ç©¶èŒƒå›´å’Œé—®é¢˜",
                "required_skills": ["analysis", "planning"],
                "agent_type": "athena",
                "estimated_time": 30,
                "status": "pending",
                "llm_generated": False
            },
            {
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": "æœé›†å’Œæ•´ç†èµ„æ–™",
                "required_skills": ["research", "data_collection"],
                "agent_type": "athena",
                "estimated_time": 90,
                "status": "pending",
                "llm_generated": False
            },
            {
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": "åˆ†æå’Œç»¼åˆä¿¡æ¯",
                "required_skills": ["analysis", "synthesis"],
                "agent_type": "athena",
                "estimated_time": 60,
                "status": "pending",
                "llm_generated": False
            },
            {
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": "æ’°å†™ç ”ç©¶æŠ¥å‘Š",
                "required_skills": ["writing", "reporting"],
                "agent_type": "apollo",
                "estimated_time": 60,
                "status": "pending",
                "llm_generated": False
            }
        ]
    
    def _decompose_coding(self, task_desc: str) -> List[Dict]:
        """ç¼–ç¨‹ä»»åŠ¡åˆ†è§£"""
        return [
            {
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": "éœ€æ±‚åˆ†æå’Œæ¶æ„è®¾è®¡",
                "required_skills": ["analysis", "architecture"],
                "agent_type": "daedalus",
                "estimated_time": 45,
                "status": "pending",
                "llm_generated": False
            },
            {
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": "æ ¸å¿ƒåŠŸèƒ½å®ç°",
                "required_skills": ["coding", "implementation"],
                "agent_type": "hephaestus",
                "estimated_time": 120,
                "status": "pending",
                "llm_generated": False
            },
            {
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": "å•å…ƒæµ‹è¯•ç¼–å†™",
                "required_skills": ["testing", "quality_control"],
                "agent_type": "themis",
                "estimated_time": 45,
                "status": "pending",
                "llm_generated": False
            },
            {
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": "ä»£ç å®¡æŸ¥å’Œä¼˜åŒ–",
                "required_skills": ["review", "optimization"],
                "agent_type": "themis",
                "estimated_time": 30,
                "status": "pending",
                "llm_generated": False
            }
        ]
    
    def _decompose_web_development(self, task_desc: str) -> List[Dict]:
        """ç½‘ç«™å¼€å‘ä»»åŠ¡åˆ†è§£"""
        return [
            {
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": "éœ€æ±‚åˆ†æå’ŒåŸå‹è®¾è®¡",
                "required_skills": ["analysis", "design"],
                "agent_type": "daedalus",
                "estimated_time": 60,
                "status": "pending",
                "llm_generated": False
            },
            {
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": "å‰ç«¯é¡µé¢å¼€å‘",
                "required_skills": ["frontend", "html", "css", "javascript"],
                "agent_type": "hephaestus",
                "estimated_time": 120,
                "status": "pending",
                "llm_generated": False
            },
            {
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": "åç«¯ API å¼€å‘",
                "required_skills": ["backend", "api", "database"],
                "agent_type": "hephaestus",
                "estimated_time": 120,
                "status": "pending",
                "llm_generated": False
            },
            {
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": "æ•°æ®åº“è®¾è®¡ä¸å®ç°",
                "required_skills": ["database", "sql"],
                "agent_type": "daedalus",
                "estimated_time": 60,
                "status": "pending",
                "llm_generated": False
            },
            {
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": "éƒ¨ç½²é…ç½®å’Œæµ‹è¯•",
                "required_skills": ["devops", "deployment", "testing"],
                "agent_type": "hephaestus",
                "estimated_time": 60,
                "status": "pending",
                "llm_generated": False
            }
        ]
    
    def _decompose_generic(self, task_desc: str) -> List[Dict]:
        """é€šç”¨ä»»åŠ¡åˆ†è§£"""
        return [
            {
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": "ç†è§£ä»»åŠ¡éœ€æ±‚å’Œç›®æ ‡",
                "required_skills": ["analysis"],
                "agent_type": "athena",
                "estimated_time": 20,
                "status": "pending",
                "llm_generated": False
            },
            {
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": "åˆ¶å®šæ‰§è¡Œè®¡åˆ’",
                "required_skills": ["planning"],
                "agent_type": "hermes",
                "estimated_time": 30,
                "status": "pending",
                "llm_generated": False
            },
            {
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": "æ‰§è¡Œæ ¸å¿ƒä»»åŠ¡",
                "required_skills": ["execution"],
                "agent_type": "hephaestus",
                "estimated_time": 90,
                "status": "pending",
                "llm_generated": False
            },
            {
                "subtask_id": str(uuid.uuid4())[:8],
                "desc": "è´¨é‡æ£€æŸ¥ä¸äº¤ä»˜",
                "required_skills": ["review", "quality_control"],
                "agent_type": "themis",
                "estimated_time": 20,
                "status": "pending",
                "llm_generated": False
            }
        ]
    
    def execute_agent_task(self, agent_type: str, task_desc: str, context: Dict = None) -> Dict:
        """
        æ‰§è¡Œ Agent ä»»åŠ¡
        
        Args:
            agent_type: Agent ç±»å‹
            task_desc: ä»»åŠ¡æè¿°
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
        
        Returns:
            æ‰§è¡Œç»“æœ
        """
        print(f"   ğŸ¤– [{agent_type}] æ‰§è¡Œï¼š{task_desc[:50]}...")
        
        # å¦‚æœæœ‰çœŸå® LLMï¼Œå¯ä»¥è°ƒç”¨å®ƒç”Ÿæˆå†…å®¹
        if self.provider in ["openai", "anthropic"] and self.api_key:
            try:
                return self._llm_execute(agent_type, task_desc, context)
            except Exception as e:
                print(f"   âš ï¸  LLM æ‰§è¡Œå¤±è´¥ï¼š{e}")
        
        # Fallback åˆ°æ¨¡æ‹Ÿæ‰§è¡Œ
        return self._mock_execute(agent_type, task_desc)
    
    def _llm_execute(self, agent_type: str, task_desc: str, context: Dict = None) -> Dict:
        """ä½¿ç”¨çœŸå® LLM æ‰§è¡Œä»»åŠ¡"""
        
        # æ„å»ºæç¤ºè¯
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ {agent_type} Agentã€‚
è¯·æ ¹æ®ä»»åŠ¡æè¿°å®Œæˆå·¥ä½œï¼Œå¹¶è¿”å›ç»“æ„åŒ–çš„ç»“æœã€‚

è¿”å›æ ¼å¼ï¼ˆJSONï¼‰ï¼š
{{
    "success": true/false,
    "output": "ä»»åŠ¡è¾“å‡ºçš„è¯¦ç»†æè¿°",
    "execution_time": æ‰§è¡Œæ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰,
    "artifacts": ["äº§å‡ºçš„æ–‡ä»¶åˆ—è¡¨"],
    "logs": ["æ‰§è¡Œæ—¥å¿—"],
    "confidence": ç½®ä¿¡åº¦ (0.0-1.0)
}}

è¯·ç¡®ä¿è¾“å‡ºä¸“ä¸šã€è¯¦ç»†ä¸”å¯æ‰§è¡Œã€‚"""

        user_prompt = f"""è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

ä»»åŠ¡æè¿°ï¼š{task_desc}
{'ä¸Šä¸‹æ–‡ï¼š' + json.dumps(context, ensure_ascii=False) if context else ''}

è¯·è¿”å› JSON æ ¼å¼çš„æ‰§è¡Œç»“æœï¼š"""

        if self.provider == "openai" and self.openai_client:
            try:
                response = self.openai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.7,
                    max_tokens=2000
                )
                
                content = response.choices[0].message.content.strip()
                result = json.loads(content)
                
                # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
                if "success" not in result:
                    result["success"] = True
                if "output" not in result:
                    result["output"] = f"[{agent_type}] å®Œæˆä»»åŠ¡ï¼š{task_desc[:50]}"
                if "execution_time" not in result:
                    result["execution_time"] = 30
                if "artifacts" not in result:
                    result["artifacts"] = []
                if "logs" not in result:
                    result["logs"] = [f"æ‰§è¡Œ {task_desc[:30]}..."]
                if "confidence" not in result:
                    result["confidence"] = 0.9
                
                return result
                
            except Exception as e:
                print(f"   âš ï¸  LLM æ‰§è¡Œå¤±è´¥ï¼š{e}")
                print("   ğŸ”„ Fallback åˆ°æ¨¡æ‹Ÿæ‰§è¡Œ")
                return self._mock_execute(agent_type, task_desc)
        
        elif self.provider == "anthropic" and self.anthropic_client:
            try:
                response = self.anthropic_client.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=2000,
                    system=system_prompt,
                    messages=[{"role": "user", "content": user_prompt}]
                )
                
                content = response.content[0].text.strip()
                result = json.loads(content)
                
                # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
                if "success" not in result:
                    result["success"] = True
                if "output" not in result:
                    result["output"] = f"[{agent_type}] å®Œæˆä»»åŠ¡ï¼š{task_desc[:50]}"
                if "execution_time" not in result:
                    result["execution_time"] = 30
                if "artifacts" not in result:
                    result["artifacts"] = []
                if "logs" not in result:
                    result["logs"] = [f"æ‰§è¡Œ {task_desc[:30]}..."]
                if "confidence" not in result:
                    result["confidence"] = 0.9
                
                return result
                
            except Exception as e:
                print(f"   âš ï¸  LLM æ‰§è¡Œå¤±è´¥ï¼š{e}")
                print("   ğŸ”„ Fallback åˆ°æ¨¡æ‹Ÿæ‰§è¡Œ")
                return self._mock_execute(agent_type, task_desc)
        
        else:
            return self._mock_execute(agent_type, task_desc)
    
    def _mock_execute(self, agent_type: str, task_desc: str) -> Dict:
        """æ¨¡æ‹Ÿæ‰§è¡Œ"""
        # æ ¹æ® Agent ç±»å‹ç”Ÿæˆä¸åŒçš„æ¨¡æ‹Ÿç»“æœ
        artifacts_map = {
            "athena": ["è°ƒç ”æŠ¥å‘Š.md", "æ•°æ®åˆ†æ.xlsx"],
            "apollo": ["å†…å®¹æ—¥å†.xlsx", "æ–‡æ¡ˆè‰ç¨¿.docx", "è§†è§‰æŒ‡å—.pdf"],
            "hephaestus": ["main.py", "tests.py", "README.md"],
            "themis": ["å®¡æ ¸æŠ¥å‘Š.md", "ä¼˜åŒ–å»ºè®®åˆ—è¡¨.txt"],
            "hermes": ["æ¶æ„è®¾è®¡.md", "æŠ€æœ¯æ–¹æ¡ˆ.docx"],
            "daedalus": ["system_design.md", "api_docs.md"],
            "muse": ["æ–‡ç« è‰ç¨¿.md", "çµæ„Ÿç¬”è®°.txt"],
            "hestia": ["ä»»åŠ¡æ¸…å•.xlsx", "è´¨é‡æŠ¥å‘Š.md"],
            "aphrodite": ["è¥é”€ç­–ç•¥.md", "å“ç‰ŒæŒ‡å—.pdf"]
        }
        
        artifacts = artifacts_map.get(agent_type, ["output.txt"])
        
        return {
            "success": True,
            "output": f"[{agent_type}] å®Œæˆä»»åŠ¡ï¼š{task_desc[:50]}",
            "execution_time": 30,
            "artifacts": artifacts,
            "logs": [f"æ‰§è¡Œ {task_desc[:30]}..."]
        }


class ExecutionLogger:
    """æ‰§è¡Œæ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, log_path: Path):
        self.log_path = log_path
        self.log_path.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ“ æ‰§è¡Œæ—¥å¿—ç³»ç»Ÿå·²åˆå§‹åŒ–ï¼š{log_path}")
    
    def start_task(self, task_id: str, task_desc: str, claw_info: Dict):
        log_entry = {
            "event": "task_start",
            "task_id": task_id,
            "task_desc": task_desc,
            "claw_info": claw_info,
            "timestamp": datetime.now().isoformat()
        }
        self._save_log(task_id, log_entry)
    
    def log_subtask_start(self, task_id: str, subtask: Dict, agent_id: str):
        log_entry = {
            "event": "subtask_start",
            "subtask_id": subtask.get("subtask_id"),
            "subtask_desc": subtask.get("desc"),
            "agent_id": agent_id,
            "timestamp": datetime.now().isoformat()
        }
        self._save_log(task_id, log_entry)
    
    def log_subtask_complete(self, task_id: str, subtask_id: str, result: Dict):
        log_entry = {
            "event": "subtask_complete",
            "subtask_id": subtask_id,
            "result": result,
            "timestamp": datetime.now().isoformat()
        }
        self._save_log(task_id, log_entry)
    
    def log_decision(self, task_id: str, decision_type: str, decision: str, rationale: str):
        log_entry = {
            "event": "decision",
            "decision_type": decision_type,
            "decision": decision,
            "rationale": rationale,
            "timestamp": datetime.now().isoformat()
        }
        self._save_log(task_id, log_entry)
    
    def log_exception(self, task_id: str, error: str, resolution: str = None):
        log_entry = {
            "event": "exception",
            "error": error,
            "resolution": resolution,
            "timestamp": datetime.now().isoformat()
        }
        self._save_log(task_id, log_entry)
    
    def complete_task(self, task_id: str, result: Dict, feedback: str = None):
        log_entry = {
            "event": "task_complete",
            "result": result,
            "feedback": feedback,
            "timestamp": datetime.now().isoformat()
        }
        self._save_log(task_id, log_entry)
    
    def _save_log(self, task_id: str, log_entry: Dict):
        log_file = self.log_path / f"{task_id}.jsonl"
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
    
    def get_task_logs(self, task_id: str) -> List[Dict]:
        log_file = self.log_path / f"{task_id}.jsonl"
        if not log_file.exists():
            return []
        
        logs = []
        with open(log_file, 'r', encoding='utf-8') as f:
            for line in f:
                logs.append(json.loads(line))
        return logs


if __name__ == "__main__":
    # æµ‹è¯• LLM é›†æˆ
    print("ğŸ§  Olympus LLM Integration Test")
    print("=" * 50)
    
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    llm = LLMClient()
    
    # æµ‹è¯•ä»»åŠ¡åˆ†è§£
    task_desc = "ä¸ºä¸€ä¸ªå°å‹åˆ›ä¸šå›¢é˜Ÿç”Ÿæˆä¸€å‘¨çš„ç¤¾äº¤åª’ä½“å†…å®¹è®¡åˆ’"
    print(f"\nğŸ“‹ ä»»åŠ¡ï¼š{task_desc}")
    
    subtasks = llm.decompose_task(task_desc)
    print(f"\nâœ… åˆ†è§£ä¸º {len(subtasks)} ä¸ªå­ä»»åŠ¡:")
    for i, st in enumerate(subtasks, 1):
        llm_generated = "ğŸ¤– LLM" if st.get("llm_generated") else "ğŸ’¾ Mock"
        print(f"   {i}. {st['desc']} ({st['agent_type']}, {st['estimated_time']}min) [{llm_generated}]")
    
    print("\nâœ… æµ‹è¯•å®Œæˆ")
