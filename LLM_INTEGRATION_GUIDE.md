# ğŸ¤– Proteus System LLM é›†æˆæŒ‡å—

> **å®Œæ•´æ”¯æŒçœŸå® LLM API è°ƒç”¨** - ä»»åŠ¡åˆ†è§£ + ä»»åŠ¡æ‰§è¡Œå…¨æµç¨‹

---

## ğŸ“‹ åŠŸèƒ½æ¦‚è§ˆ

Proteus System å·²å®Œæ•´æ”¯æŒçœŸå® LLM é›†æˆï¼š

| åŠŸèƒ½æ¨¡å— | æ¨¡æ‹Ÿæ¨¡å¼ | çœŸå® LLM | çŠ¶æ€ |
|---------|---------|---------|------|
| **ä»»åŠ¡åˆ†è§£** | âœ… å…³é”®è¯åŒ¹é… | âœ… OpenAI/Anthropic | âœ… å®Œæˆ |
| **ä»»åŠ¡æ‰§è¡Œ** | âœ… æ¨¡æ¿ç”Ÿæˆ | âœ… OpenAI/Anthropic | âœ… å®Œæˆ |
| **è‡ªåŠ¨ Fallback** | - | âœ… API å¤±è´¥é™çº§ | âœ… å®Œæˆ |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼ 1: ä½¿ç”¨ OpenAI

```bash
# é…ç½®ç¯å¢ƒå˜é‡
export OLYMPUS_LLM_PROVIDER=openai
export OPENAI_API_KEY=sk-your-api-key-here

# è¿è¡Œæ¼”ç¤º
python3 demo_evolution.py
```

**é¢„æœŸè¾“å‡º**ï¼š
```
ğŸ§  OpenAI Client å·²åˆå§‹åŒ– (æ¨¡å‹ï¼šgpt-4o)
ğŸ“‹ ä»»åŠ¡ï¼šä¸ºä¸€ä¸ªå°å‹åˆ›ä¸šå›¢é˜Ÿç”Ÿæˆä¸€å‘¨çš„ç¤¾äº¤åª’ä½“å†…å®¹è®¡åˆ’

ğŸ¤– ä½¿ç”¨ LLM åˆ†è§£ä»»åŠ¡...
âœ… åˆ†è§£ä¸º 5 ä¸ªå­ä»»åŠ¡:
   1. è°ƒç ”ç›®æ ‡å—ä¼—å’Œè¡Œä¸šè¶‹åŠ¿ (athena, 45min) [ğŸ¤– LLM]
   2. åˆ¶å®šå†…å®¹ä¸»é¢˜å’Œå‘å¸ƒæ—¥å† (apollo, 30min) [ğŸ¤– LLM]
   3. æ’°å†™æ¯æ—¥æ–‡æ¡ˆè‰ç¨¿ (apollo, 90min) [ğŸ¤– LLM]
   4. è®¾è®¡è§†è§‰é£æ ¼å’Œé…å›¾å»ºè®® (hephaestus, 60min) [ğŸ¤– LLM]
   5. è´¨é‡å®¡æ ¸ä¸ä¼˜åŒ– (themis, 30min) [ğŸ¤– LLM]

ğŸ¤ [Hub] å¼€å§‹æ‰§è¡Œä»»åŠ¡...
ğŸ¤– [athena] æ‰§è¡Œï¼šè°ƒç ”ç›®æ ‡å—ä¼—... [ğŸ¤– LLM]
```

### æ–¹å¼ 2: ä½¿ç”¨ Anthropic

```bash
export OLYMPUS_LLM_PROVIDER=anthropic
export ANTHROPIC_API_KEY=sk-ant-your-api-key-here

python3 demo_evolution.py
```

### æ–¹å¼ 3: æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆæ— éœ€ API Keyï¼‰

```bash
export OLYMPUS_LLM_PROVIDER=mock
# æˆ–ä¸è®¾ç½®ä»»ä½•ç¯å¢ƒå˜é‡

python3 demo_evolution.py
```

---

## ğŸ”§ é…ç½®é€‰é¡¹

### ç¯å¢ƒå˜é‡

| å˜é‡å | è¯´æ˜ | ç¤ºä¾‹å€¼ |
|--------|------|--------|
| `OLYMPUS_LLM_PROVIDER` | LLM æä¾›å•† | `openai` / `anthropic` / `mock` |
| `OPENAI_API_KEY` | OpenAI API Key | `sk-...` |
| `ANTHROPIC_API_KEY` | Anthropic API Key | `sk-ant-...` |

### .env æ–‡ä»¶é…ç½®

```bash
# .env æ–‡ä»¶

# é€‰æ‹© LLM æä¾›å•†
OLYMPUS_LLM_PROVIDER=openai

# OpenAI é…ç½®
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxx

# Anthropic é…ç½®ï¼ˆå¦‚æœé€‰æ‹© anthropicï¼‰
# ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxx
```

---

## ğŸ“Š LLM è°ƒç”¨æµç¨‹

### 1. ä»»åŠ¡åˆ†è§£é˜¶æ®µ

```python
# hub.py
subtasks = self.llm.decompose_task(task_desc, context)

# llm_integration.py
if self.provider in ["openai", "anthropic"] and self.api_key:
    return self._llm_decompose(task_desc, context)  # çœŸå® LLM
else:
    return self._mock_decompose(task_desc)  # æ¨¡æ‹Ÿæ¨¡å¼
```

**LLM æç¤ºè¯ç¤ºä¾‹**ï¼š
```
ç³»ç»Ÿï¼šä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»»åŠ¡è§„åˆ’ä¸“å®¶ã€‚è¯·å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¯æ‰§è¡Œçš„å­ä»»åŠ¡ã€‚

ç”¨æˆ·ï¼šè¯·åˆ†è§£ä»¥ä¸‹ä»»åŠ¡ï¼š
ä»»åŠ¡ï¼šä¸ºä¸€ä¸ªå°å‹åˆ›ä¸šå›¢é˜Ÿç”Ÿæˆä¸€å‘¨çš„ç¤¾äº¤åª’ä½“å†…å®¹è®¡åˆ’

è¯·è¿”å›å­ä»»åŠ¡åˆ—è¡¨ï¼ˆJSON æ•°ç»„æ ¼å¼ï¼‰ï¼š
```

**LLM è¿”å›**ï¼š
```json
[
  {
    "desc": "è°ƒç ”ç›®æ ‡å—ä¼—å’Œè¡Œä¸šè¶‹åŠ¿",
    "required_skills": ["research", "analysis"],
    "agent_type": "athena",
    "estimated_time": 45
  },
  ...
]
```

### 2. ä»»åŠ¡æ‰§è¡Œé˜¶æ®µ

```python
# hub.py
result = self.llm.execute_agent_task(
    agent_type,
    subtask["desc"],
    context=self.memory.working.get_context()
)

# llm_integration.py
if self.provider in ["openai", "anthropic"] and self.api_key:
    return self._llm_execute(agent_type, task_desc, context)  # çœŸå® LLM
else:
    return self._mock_execute(agent_type, task_desc)  # æ¨¡æ‹Ÿæ¨¡å¼
```

**LLM æç¤ºè¯ç¤ºä¾‹**ï¼š
```
ç³»ç»Ÿï¼šä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ athena Agentï¼ˆç ”ç©¶ä¸“å®¶ï¼‰ã€‚
è¯·æ ¹æ®ä»»åŠ¡æè¿°å®Œæˆå·¥ä½œï¼Œå¹¶è¿”å›ç»“æ„åŒ–çš„ç»“æœã€‚

è¿”å›æ ¼å¼ï¼ˆJSONï¼‰ï¼š
{
    "success": true/false,
    "output": "ä»»åŠ¡è¾“å‡ºçš„è¯¦ç»†æè¿°",
    "execution_time": æ‰§è¡Œæ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰,
    "artifacts": ["äº§å‡ºçš„æ–‡ä»¶åˆ—è¡¨"],
    "logs": ["æ‰§è¡Œæ—¥å¿—"],
    "confidence": ç½®ä¿¡åº¦ (0.0-1.0)
}

ç”¨æˆ·ï¼šè¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
ä»»åŠ¡æè¿°ï¼šè°ƒç ”ç›®æ ‡å—ä¼—å’Œè¡Œä¸šè¶‹åŠ¿
```

**LLM è¿”å›**ï¼š
```json
{
    "success": true,
    "output": "å®Œæˆç›®æ ‡å—ä¼—è°ƒç ”ï¼Œè¯†åˆ«å‡º 3 ä¸ªä¸»è¦ç”¨æˆ·ç¾¤ä½“...",
    "execution_time": 45,
    "artifacts": ["è°ƒç ”æŠ¥å‘Š.md", "ç”¨æˆ·ç”»åƒ.xlsx", "æ•°æ®åˆ†æ.csv"],
    "logs": ["æ”¶é›†è¡Œä¸šæ•°æ®...", "åˆ†æç”¨æˆ·è¡Œä¸º...", "ç”ŸæˆæŠ¥å‘Š..."],
    "confidence": 0.92
}
```

---

## ğŸ§ª æµ‹è¯•éªŒè¯

### æµ‹è¯• 1: æ£€æŸ¥ LLM è¿æ¥

```bash
python3 -c "
from core.llm_integration import LLMClient

llm = LLMClient()
print(f'æä¾›å•†ï¼š{llm.provider}')
print(f'API Key: {\"å·²é…ç½®\" if llm.api_key else \"æœªé…ç½®\"}')
print(f'OpenAI å®¢æˆ·ç«¯ï¼š{\"âœ…\" if llm.openai_client else \"âŒ\"}')
print(f'Anthropic å®¢æˆ·ç«¯ï¼š{\"âœ…\" if llm.anthropic_client else \"âŒ\"}')
"
```

### æµ‹è¯• 2: æµ‹è¯•ä»»åŠ¡åˆ†è§£

```bash
python3 -c "
from core.llm_integration import LLMClient

llm = LLMClient()
task = 'ä¸º AI åˆ›ä¸šå…¬å¸å†™ä¸€ä»½å•†ä¸šè®¡åˆ’ä¹¦'
subtasks = llm.decompose_task(task)

print(f'åˆ†è§£ä¸º {len(subtasks)} ä¸ªå­ä»»åŠ¡:')
for i, st in enumerate(subtasks, 1):
    llm_tag = 'ğŸ¤– LLM' if st.get('llm_generated') else 'ğŸ’¾ Mock'
    print(f'{i}. {st[\"desc\"]} ({st[\"agent_type\"]}) [{llm_tag}]')
"
```

### æµ‹è¯• 3: æµ‹è¯•ä»»åŠ¡æ‰§è¡Œ

```bash
python3 -c "
from core.llm_integration import LLMClient

llm = LLMClient()
result = llm.execute_agent_task(
    'athena',
    'è°ƒç ”ä¸­å›½æ–°èƒ½æºæ±½è½¦å¸‚åœº',
    context={'industry': 'EV', 'region': 'China'}
)

print(f'æ‰§è¡Œç»“æœ:')
print(f'  æˆåŠŸï¼š{result[\"success\"]}')
print(f'  è¾“å‡ºï¼š{result[\"output\"][:100]}...')
print(f'  æ—¶é—´ï¼š{result[\"execution_time\"]}åˆ†é’Ÿ')
print(f'  äº§ç‰©ï¼š{result[\"artifacts\"]}')
print(f'  ç½®ä¿¡åº¦ï¼š{result[\"confidence\"]}')
"
```

---

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | æ¨¡æ‹Ÿæ¨¡å¼ | çœŸå® LLM |
|------|---------|---------|
| **ä»»åŠ¡åˆ†è§£è´¨é‡** | â­â­â­ å›ºå®šæ¨¡æ¿ | â­â­â­â­â­ æ™ºèƒ½é€‚é… |
| **ä»»åŠ¡æ‰§è¡Œè´¨é‡** | â­â­ é€šç”¨å›å¤ | â­â­â­â­â­ ä¸“ä¸šè¾“å‡º |
| **å“åº”é€Ÿåº¦** | âš¡ <1 ç§’ | ğŸ¢ 5-30 ç§’ |
| **æˆæœ¬** | ğŸ’° å…è´¹ | ğŸ’°ğŸ’° API è´¹ç”¨ |
| **é€‚ç”¨åœºæ™¯** | å¼€å‘æµ‹è¯• | ç”Ÿäº§ç¯å¢ƒ |

---

## ğŸ”’ å®‰å…¨ä¸æˆæœ¬æ§åˆ¶

### API Key å®‰å…¨

```bash
# âœ… æ­£ç¡®ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡
export OPENAI_API_KEY="sk-..."

# âœ… æ­£ç¡®ï¼šä½¿ç”¨ .env æ–‡ä»¶ï¼ˆå·²åœ¨ .gitignore ä¸­ï¼‰
cp .env.example .env
nano .env

# âŒ é”™è¯¯ï¼šç¡¬ç¼–ç åœ¨ä»£ç ä¸­
api_key = "sk-xxxxx"  # ä¸è¦è¿™æ ·åšï¼
```

### æˆæœ¬ä¼°ç®—

**OpenAI GPT-4o å®šä»·**ï¼š
- è¾“å…¥ï¼š$0.005 / 1K tokens
- è¾“å‡ºï¼š$0.015 / 1K tokens

**ç¤ºä¾‹ä»»åŠ¡æˆæœ¬**ï¼š
```
ä»»åŠ¡åˆ†è§£ï¼š~500 tokens â†’ $0.01
ä»»åŠ¡æ‰§è¡Œï¼š~1000 tokens â†’ $0.02
æ€»æˆæœ¬ï¼š~$0.03 / ä»»åŠ¡
```

**æœˆåº¦é¢„ç®—å»ºè®®**ï¼š
- å¼€å‘æµ‹è¯•ï¼š$5-10/æœˆ
- å°è§„æ¨¡ä½¿ç”¨ï¼š$30-50/æœˆ
- ç”Ÿäº§ç¯å¢ƒï¼š$100+/æœˆ

### é™æµä¿æŠ¤

```python
# è‡ªåŠ¨é‡è¯•æœºåˆ¶
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential())
def call_llm():
    return client.chat.completions.create(...)
```

---

## âš ï¸ æ•…éšœæ’é™¤

### é—®é¢˜ 1: LLM åˆå§‹åŒ–å¤±è´¥

```
âš ï¸  LLM è°ƒç”¨å¤±è´¥ï¼šOpenAI API key not found
ğŸ”„ Fallback åˆ°æ¨¡æ‹Ÿæ¨¡å¼
```

**è§£å†³**ï¼š
```bash
# æ£€æŸ¥ç¯å¢ƒå˜é‡
echo $OPENAI_API_KEY

# å¦‚æœä¸ºç©ºï¼Œè®¾ç½®å®ƒ
export OPENAI_API_KEY="sk-..."

# æˆ–æ£€æŸ¥ .env æ–‡ä»¶
cat .env
```

### é—®é¢˜ 2: API è°ƒç”¨è¶…æ—¶

```
âš ï¸  LLM è°ƒç”¨å¤±è´¥ï¼šConnection timeout
ğŸ”„ Fallback åˆ°æ¨¡æ‹Ÿæ¨¡å¼
```

**è§£å†³**ï¼š
```bash
# æ£€æŸ¥ç½‘ç»œ
curl https://api.openai.com

# ä½¿ç”¨ä»£ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
export HTTP_PROXY="http://proxy:port"
export HTTPS_PROXY="http://proxy:port"

# æˆ–åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ¨¡å¼
export OLYMPUS_LLM_PROVIDER=mock
```

### é—®é¢˜ 3: JSON è§£æå¤±è´¥

```
âš ï¸  LLM è¿”å›æ ¼å¼é”™è¯¯
ğŸ”„ Fallback åˆ°æ¨¡æ‹Ÿæ¨¡å¼
```

**è§£å†³**ï¼š
- ç³»ç»Ÿå·²è‡ªåŠ¨å¤„ç†ï¼ˆæå– JSONã€ä¿®å¤æ ¼å¼ï¼‰
- å¦‚é¢‘ç¹å‘ç”Ÿï¼Œæ£€æŸ¥ LLM æ¨¡å‹ç‰ˆæœ¬
- æˆ–é™ä½ temperature å‚æ•°

---

## ğŸ¯ æœ€ä½³å®è·µ

### 1. å¼€å‘é˜¶æ®µ

```bash
# ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆå¿«é€Ÿè¿­ä»£ï¼‰
export OLYMPUS_LLM_PROVIDER=mock

# å¿«é€Ÿæµ‹è¯•
python3 demo.py
```

### 2. æµ‹è¯•é˜¶æ®µ

```bash
# ä½¿ç”¨çœŸå® LLMï¼ˆå°æµé‡ï¼‰
export OLYMPUS_LLM_PROVIDER=openai
export OPENAI_API_KEY="sk-..."

# éªŒè¯åŠŸèƒ½
python3 demo_evolution.py
```

### 3. ç”Ÿäº§ç¯å¢ƒ

```bash
# é…ç½®å®Œæ•´ç¯å¢ƒå˜é‡
export OLYMPUS_LLM_PROVIDER=openai
export OPENAI_API_KEY="sk-..."
export PYTHONPATH="${PYTHONPATH}:$(pwd)"

# è¿è¡Œä¸»ç¨‹åº
python3 main.py
```

### 4. æ—¥å¿—è®°å½•

```python
# è®°å½• LLM è°ƒç”¨
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("ProteusLLM")

logger.info(f"LLM Provider: {provider}")
logger.info(f"Task: {task_desc[:50]}...")
logger.info(f"Result: {result['success']}")
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) - ç‹¬ç«‹éƒ¨ç½²æŒ‡å—
- [LLM_SETUP.md](LLM_SETUP.md) - LLM é…ç½®æŒ‡å—
- [README.md](README.md) - é¡¹ç›®è¯´æ˜
- [examples/](examples/) - ä½¿ç”¨ç¤ºä¾‹

---

## ğŸ™‹ FAQ

### Q: å¿…é¡»é…ç½® API Key å—ï¼Ÿ
**A**: ä¸å¿…é¡»ã€‚ç³»ç»Ÿæ”¯æŒæ¨¡æ‹Ÿæ¨¡å¼ï¼Œæ— éœ€ API Key å³å¯è¿è¡Œã€‚ä½†çœŸå® LLM èƒ½æä¾›æ›´é«˜è´¨é‡çš„è¾“å‡ºã€‚

### Q: æ”¯æŒå“ªäº› LLM æä¾›å•†ï¼Ÿ
**A**: ç›®å‰æ”¯æŒ OpenAI (GPT-4/GPT-4o) å’Œ Anthropic (Claude 3.5)ã€‚æœªæ¥å¯èƒ½æ”¯æŒæ›´å¤šã€‚

### Q: å¯ä»¥åœ¨æœ¬åœ°è¿è¡Œ LLM å—ï¼Ÿ
**A**: å½“å‰ç‰ˆæœ¬ä¸æ”¯æŒã€‚å¦‚éœ€æœ¬åœ° LLMï¼Œå¯è€ƒè™‘ Ollama + OpenAI å…¼å®¹ APIã€‚

### Q: API è°ƒç”¨å¤±è´¥æ€ä¹ˆåŠï¼Ÿ
**A**: ç³»ç»Ÿä¼šè‡ªåŠ¨ Fallback åˆ°æ¨¡æ‹Ÿæ¨¡å¼ï¼Œç¡®ä¿ç¨‹åºä¸ä¼šå´©æºƒã€‚

### Q: å¦‚ä½•æŸ¥çœ‹ LLM è°ƒç”¨æ—¥å¿—ï¼Ÿ
**A**: æ—¥å¿—ä¿å­˜åœ¨ `logs/llm_calls/` ç›®å½•ï¼ŒåŒ…å«è¯·æ±‚å’Œå“åº”è¯¦æƒ…ã€‚

---

*Last updated: 2026-02-25*

> **æç¤º**: é¦–æ¬¡ä½¿ç”¨å»ºè®®ä»æ¨¡æ‹Ÿæ¨¡å¼å¼€å§‹ï¼Œç†Ÿæ‚‰ç³»ç»Ÿåå†é…ç½®çœŸå® APIã€‚
